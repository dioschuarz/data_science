{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-google-genai langchain_postgres wikiscraper \"psycopg[binary,pool]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import (ChatGoogleGenerativeAI,\n",
    "                                    GoogleGenerativeAIEmbeddings)\n",
    "\n",
    "import re\n",
    "import tiktoken\n",
    "import wikipedia as wk\n",
    "# import wikiscraper as ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(load_dotenv(\"./../../../../../envs/invest.env\"))\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\",\n",
    "                             api_key=GOOGLE_API_KEY,\n",
    "                             temperature=0,\n",
    "                             )\n",
    "\n",
    "EMBEDDING = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",\n",
    "                                         google_api_key=GOOGLE_API_KEY)\n",
    "# ENCODING = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language code is: pt\n",
      "The article slug is: Nikola_Tesla\n",
      "The article title is: Nikola Tesla\n",
      "Nikola Tesla (em sérvio: Никола Тесла; pronunciação sérvia: [nǐkola têsla]; Smiljan, Império Austría\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://pt.wikipedia.org/wiki/Nikola_Tesla\"\n",
    "\n",
    "# Regular expression to match the language code\n",
    "L_PATTERN = r\"https://(.*?)\\.wikipedia\\.org/\"\n",
    "T_PATTERN = r\"/wiki/([^#]+)\"\n",
    "\n",
    "l_match = re.search(L_PATTERN, URL)\n",
    "t_match = re.search(T_PATTERN, URL)\n",
    "if l_match:\n",
    "    LANGUAGE_CODE = l_match.group(1)\n",
    "    print(f\"The language code is: {LANGUAGE_CODE}\")\n",
    "else:\n",
    "    LANGUAGE_CODE = 'en'\n",
    "    print(\"No language code found in the URL using default 'en'.\")\n",
    "if t_match:\n",
    "    SLUG = t_match.group(1)\n",
    "    TITLE = SLUG.replace(\"_\",\" \")\n",
    "    print(f\"The article slug is: {SLUG}\")\n",
    "    print(f\"The article title is: {TITLE}\")\n",
    "else:\n",
    "    print(\"No slug found in the URL, please provide a full URL of an article.\")\n",
    "\n",
    "wk.set_lang(LANGUAGE_CODE)\n",
    "# page = ws.searchBySlug(SLUG)\n",
    "# TITLE = page.getTitle()\n",
    "\n",
    "# Specify the title of the Wikipedia page\n",
    "wiki = wk.page(TITLE)\n",
    "\n",
    "# Extract the plain text content of the page, excluding images, tables, and other data.\n",
    "RAW_TEXT = wiki.content\n",
    "RAW_DOC = Document(page_content=RAW_TEXT)\n",
    "\n",
    "print(RAW_DOC.page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Nikola Tesla (em sérvio: Никола Тесла; pronunciação sérvia: [nǐkola têsla]; Smiljan, Império Austríaco, 10 de julho de 1856 — Nova Iorque, 7 de janeiro de 1943) foi um inventor, engenheiro eletrotécnico e engenheiro mecânico sérvio, mais conhecido por suas contribuições ao projeto do moderno sistema de fornecimento de eletricidade em corrente alternada (CA).\n",
      "Nascido e criado no Império Austríaco, Tesla estudou engenharia e física na década de 1870 sem se formar, e ganhou experiência prática no início da década de 1880 trabalhando em telefonia e na Continental Edison, na nova indústria de energia elétrica. Em 1884, emigrou para os Estados Unidos e se naturalizou cidadão americano. Ele trabalhou por um curto período na Edison Machine Works, em Nova Iorque, antes de começar por conta própria. Com a ajuda de parceiros para financiar e comercializar suas ideias, Tesla montou laboratórios e empresas em Nova Iorque para desenvolver uma variedade de dispositivos elétricos e mecânicos. Seu motor de indução de corrente alternada e patentes relacionadas ao sistema polifásico, licenciadas pela Westinghouse Electric em 1888, lhe renderam uma quantia considerável de dinheiro e se tornaram a pedra angular do sistema polifásico que a empresa acabou comercializando.\n",
      "Tentando desenvolver invenções que pudesse patentear e comercializar, Tesla conduziu uma série de experimentos com osciladores/geradores mecânicos, tubos de descarga elétrica e radiografia. Ele também construiu um barco de controle remoto, um dos primeiros já exibidos. Tesla tornou-se conhecido como inventor e demonstrou suas realizações para celebridades e patronos ricos em seu laboratório, e destacou-se por seu talento em palestras públicas. Durante a década de 1890, Tesla seguiu suas ideias para iluminação sem fio e distribuição mundial de energia elétrica sem fio em seus experimentos de alta tensão e alta frequência em Nova Iorque e Colorado Springs. Em 1893, ele fez pronunciamentos sobre a possibilidade de comunicação sem fio com seus dispositivos. Tesla tentou colocar essas ideias em uso prático em seu projeto inacabado da Wardenclyffe Tower, uma transmissora sem fio intercontinental de comunicações e energia, mas ficou sem dinheiro antes que pudesse concluí-lo.\n",
      "Depois de Wardenclyffe, Tesla experimentou uma série de invenções nas décadas de 1910 e 1920 com graus variados de sucesso. Tendo gasto a maior parte de seu dinheiro, Tesla morava em uma série de hotéis de Nova Iorque, deixando para trás contas não pagas. Ele morreu na cidade de Nova Iorque em janeiro de 1943. O trabalho de Tesla caiu em relativa obscuridade após sua morte, até 1960, quando a Conferência Geral de Pesos e Medidas nomeou a unidade SI de densidade de fluxo magnético como tesla em sua homenagem. Houve um ressurgimento do interesse popular em Tesla desde os anos 1990.\n",
      "\n",
      "\n",
      "== Juventude ==\n",
      "\n",
      "Nikola Tesla nasceu de família sérvia na aldeia Smiljan, condado de Lika, no Império Austríaco, em 10 de julho de 1856. Seu pai, Milutin Tesla (1819–1879 ), era um padre ortodoxo sérvio. A mãe de Tesla, Đuka Tesla (née Mandić; 1822-1892), cujo pai também era um padre ortodoxo sérvio, tinha um talento para fabricar ferramentas artesanais, aparelhos mecânicos e a capacidade de memorizar poemas épicos sérvios. Đuka nunca recebeu uma educação formal. Tesla creditou sua memória eidética e habilidades criativas à genética e influência de sua mãe. Os progenitores de Tesla eram do oeste da Sérvia, perto de Montenegro.\n",
      "Tesla era o quarto de cinco filhos. Ele tinha três irmãs, Milka, Angelina e Marica, e um irmão mais velho chamado Dane, morto em um acidente de cavalo quando Tesla tinha cinco anos. Em 1861, Tesla frequentou a escola primária em Smiljan, onde estudou alemão, aritmética e religião. Em 1862, a família dele mudou-se para a vizinha Gospić, Lika onde o seu pai trabalhou como pároco. Nikola concluiu o ensino fundamental, seguido pelo ensino médio. Em 1870, Tesla mudou-se para o norte, para Karlovac para cursar o ensino médio no Ginásio Real Superior. As aulas eram ministradas em alemão, pois era uma escola dentro da Fronteira Militar Austro-Húngara.\n",
      "Tesla escreveu mais tarde que se interessou em demonstrações de eletricidade por conta de seu professor de física. Tesla observou que tais demonstrações desse \"fenômeno misterioso\" o fizeram querer \"conhecer mais essa força maravilhosa\". Tesla era capaz de realizar cálculo integral em sua cabeça, o que levou seus professores a acreditar que ele estava trapaceando. Ele terminou o período escolar de quatro anos em três anos e se formou em 1873.\n",
      "\n",
      "Em 1873, Tesla retornou a Smiljan. Logo após sua chegada, ele contraiu cólera, ficou de cama por nove meses e quase morreu várias vezes. O pai de Tesla, em um momento de desespero (e que originalmente queria que ele ingressasse no sacerdócio), prometeu mandá-lo para a melhor escola de engenharia se ele se recuperasse da doença.\n",
      "Em 1874, Tesla evitou o recrutamento para o Exército Austro-Húngaro em Smiljan fugindo para o sudeste de Lika até Tomingaj, perto de Gračac. Lá, ele explorou as montanhas vestindo trajes de caçador. Tesla disse que esse contato com a natureza o tornou mais forte, tanto física quanto mentalmente. Ele leu muitos livros enquanto estava em Tomingaj e depois disse que as obras de Mark Twain o ajudaram a se recuperar milagrosamente de sua doença anterior.\n",
      "Em 1875, Tesla se matriculou no Politécnico Austríaco em Graz, na Áustria, após conseguir uma bolsa. Durante seu primeiro ano, Tesla nunca perdeu uma aula, obteve as notas mais altas possíveis, passou em nove exames (quase o dobro do necessário), fundou um clube cultural sérvio e até recebeu uma carta de recom\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=2048,\n",
    "                                  chunk_overlap=100)\n",
    "DOCS = text_splitter.split_documents([RAW_DOC])\n",
    "\n",
    "print(len(DOCS))\n",
    "print(DOCS[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36847886-fb5f-44b3-8092-eb48c53613cd',\n",
       " 'c51b451f-8239-41c0-9fe2-b19a4d29e11e',\n",
       " '9fa42f11-ad70-4686-960a-8954477d3b84',\n",
       " '235a7e31-50d1-4c48-a54a-257718969085',\n",
       " 'f6e4b074-2736-400c-a8e5-8200ee9c1238',\n",
       " '3757acbe-0521-4f60-9c44-f63ad57487ea',\n",
       " '64da0035-c94d-4e8b-bc73-0af2efc9488b',\n",
       " '9edb6ad9-6058-475b-a000-a406118268d6',\n",
       " '8f9ce5a1-c1cd-47b6-ba4d-d5d9869efd12',\n",
       " '3a9ab509-3de9-4bca-83eb-8666c3290813',\n",
       " '850e7d2d-337a-45c0-9742-d30ece85a06c',\n",
       " '4d311c30-74e4-44b4-8988-6a115764a3bd']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:5432/wiki_summarizer\"  # Uses psycopg3!\n",
    "collection_name = \"my_docs\"\n",
    "\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=EMBEDDING,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "vector_store.add_documents(DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
    ")\n",
    "\n",
    "map_chain = map_prompt | LLM | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "map_template = (\n",
    "\"\"\"\n",
    "The following is a set of documents:\n",
    "\n",
    "{docs}\n",
    "\n",
    "Based on this list of docs, please identify the main themes \n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Also available via the hub: `hub.pull(\"rlm/reduce-prompt\")`\n",
    "reduce_template = (\n",
    "\"\"\"\n",
    "The following is a set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary\n",
    "of the main themes.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "map_prompt = ChatPromptTemplate([(\"human\", map_template)])\n",
    "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "\n",
    "map_chain = map_prompt | LLM | StrOutputParser()\n",
    "reduce_chain = reduce_prompt | LLM | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reduce size of chunks\"\"\"\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "REDUCE_TEMPLATE = (\n",
    "    \"\"\"\n",
    "    The following is a set of summaries:\n",
    "\n",
    "    {docs}\n",
    "\n",
    "    Take these and distill it into a final, consolidated summary\n",
    "    using approximately {token_max} words, with a margin of 100 words\n",
    "    for less or more.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create the prompt chain\n",
    "REDUCE_PROMPT = ChatPromptTemplate([(\"human\", REDUCE_TEMPLATE)])\n",
    "reduce_chain = REDUCE_PROMPT | LLM | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Graph to Orchestrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds/intel/oneapi/intelpython/envs/venvia/lib/python3.11/site-packages/langgraph/graph/graph.py:37: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langgraph.pregel import Channel, Pregel\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build LangGraph workflow\"\"\"\n",
    "import operator\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain.chains.combine_documents.reduce import (\n",
    "    acollapse_docs,\n",
    "    split_list_of_docs,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "from backend.app.build.models import ENCODING\n",
    "# from backend.app.build.models import LLM\n",
    "# from backend.app.build.chains.map import map_chain\n",
    "# from backend.app.build.chains.reduce import reduce_chain\n",
    "\n",
    "\n",
    "def length_function(documents: List[Document]) -> int:\n",
    "    \"\"\"Get number of tokens for input contents.\"\"\"\n",
    "    return sum(LLM.get_num_tokens(doc.page_content) for doc in documents)\n",
    "\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    \"\"\"\n",
    "    Overall state of the main graph. It will contain the\n",
    "    input document contents, corresponding summaries,\n",
    "    and a final summary.\n",
    "    \"\"\"\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the summaries we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    contents: List[str]\n",
    "    token_max: int\n",
    "    summaries: Annotated[list, operator.add]\n",
    "    collapsed_summaries: List[Document]\n",
    "    final_summary: str\n",
    "\n",
    "\n",
    "class SummaryState(TypedDict):\n",
    "    \"\"\"\n",
    "    The state of the node that we will \"map\" all\n",
    "    documents to in order to generate summaries\n",
    "    \"\"\"\n",
    "    content: str\n",
    "    token_max: int\n",
    "\n",
    "\n",
    "def should_collapse(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    \"\"\"\n",
    "    Determine whether to collapse the summaries or generate the final summary.\n",
    "\n",
    "    This function acts as a conditional edge in the graph, deciding the next\n",
    "    step based on the number of tokens in the collapsed summaries.\n",
    "\n",
    "    Args:\n",
    "        state (OverallState): The current state of the graph, containing the\n",
    "                              collapsed summaries and the maximum\n",
    "                              allowed tokens.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "            The next step in the process, either to collapse the summaries\n",
    "            or to generate the final summary.\n",
    "    \"\"\"\n",
    "    num_tokens = length_function(state[\"collapsed_summaries\"])\n",
    "    if num_tokens > state[\"token_max\"]:\n",
    "        return \"collapse_summaries\"\n",
    "    else:\n",
    "        return \"generate_final_summary\"\n",
    "    \n",
    "\n",
    "def collect_summaries(state: OverallState) -> dict:\n",
    "    \"\"\"\n",
    "    Collect summaries from the state.\n",
    "\n",
    "    Args:\n",
    "        state (OverallState): The current state containing summaries\n",
    "                              and token_max.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with collapsed summaries and token_max.\n",
    "    \"\"\"\n",
    "    print(\"COLLECT_SUMMARIES\")\n",
    "    summaries = []\n",
    "    for summary in state[\"summaries\"]:\n",
    "        tokens = ENCODING.encode(summary)\n",
    "        print(\"Used:\", len(tokens),\n",
    "              \"Expected:\", state[\"token_max\"])\n",
    "        summaries.append(Document(summary))\n",
    "    return {\n",
    "        \"collapsed_summaries\": summaries,\n",
    "        \"token_max\": state[\"token_max\"]\n",
    "    }\n",
    "\n",
    "\n",
    "async def collapse_summaries(state: OverallState) -> dict:\n",
    "    \"\"\"\n",
    "    Collapse summaries into smaller chunks if they exceed the token limit.\n",
    "\n",
    "    Args:\n",
    "        state (OverallState): The current state containing collapsed summaries\n",
    "                              and token_max.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with updated collapsed summaries and token_max.\n",
    "    \"\"\"\n",
    "    doc_lists = split_list_of_docs(\n",
    "        state[\"collapsed_summaries\"],\n",
    "        length_function,\n",
    "        state[\"token_max\"]*(3)\n",
    "    )\n",
    "    print(\"COLLAPSE_SUMMARIES\")\n",
    "    for doc in doc_lists:\n",
    "        tokens = ENCODING.encode(doc[0].page_content)\n",
    "        print(\"Used:\", len(tokens),\n",
    "              \"Expected:\", state[\"token_max\"])\n",
    "    results = []\n",
    "    for doc_list in doc_lists:\n",
    "        req = {\"docs\": doc_list[0],\n",
    "               \"token_max\": state[\"token_max\"]*(2)}\n",
    "        results.append(\n",
    "            Document(await reduce_chain.ainvoke(req))\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"collapsed_summaries\": results,\n",
    "        \"token_max\": state[\"token_max\"]\n",
    "    }\n",
    "\n",
    "\n",
    "async def generate_summary(state: SummaryState) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a summary for a given document.\n",
    "\n",
    "    Args:\n",
    "        state (SummaryState): The current state containing content\n",
    "                              and token_max.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with generated summaries and token_max.\n",
    "    \"\"\"\n",
    "    print(\"GENERATE_SUMMARY\")\n",
    "    text_splitter = TokenTextSplitter(chunk_size=state[\"token_max\"]*4,\n",
    "                                      chunk_overlap=100)\n",
    "    raw_doc = Document(state[\"content\"])\n",
    "    docs = text_splitter.split_documents([raw_doc])\n",
    "    response = []\n",
    "    for doc in docs:\n",
    "        req = {\"docs\": doc,\n",
    "               \"token_max\": state[\"token_max\"]*2}\n",
    "        text = await reduce_chain.ainvoke(req)\n",
    "        response.append(text)\n",
    "        tokens = ENCODING.encode(text)\n",
    "        print(\"Used:\", len(tokens),\n",
    "              \"Expected:\", state[\"token_max\"])\n",
    "    return {\n",
    "        \"summaries\": response,\n",
    "        \"token_max\": state[\"token_max\"]\n",
    "    }\n",
    "\n",
    "\n",
    "async def generate_final_summary(state: OverallState) -> dict:\n",
    "    \"\"\"\n",
    "    Generate the final summary from collapsed summaries.\n",
    "\n",
    "    Args:\n",
    "        state (OverallState): The current state containing collapsed summaries\n",
    "                              and token_max.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the final summary and token_max.\n",
    "    \"\"\"\n",
    "    print(\"GENERATE_FINAL_SUMMARY\")\n",
    "    for doc in state[\"collapsed_summaries\"]:\n",
    "        tokens = ENCODING.encode(doc.page_content)\n",
    "        print(\"Used:\", len(tokens),\n",
    "              \"Expected:\", state[\"token_max\"])\n",
    "    req = {\"docs\": state[\"collapsed_summaries\"],\n",
    "           \"token_max\": state[\"token_max\"]}\n",
    "    response = await reduce_chain.ainvoke(req)\n",
    "    return {\n",
    "        \"final_summary\": response,\n",
    "        \"token_max\": state[\"token_max\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def map_summaries(state: OverallState) -> list:\n",
    "    \"\"\"\n",
    "    Define the logic to map out over the documents.\n",
    "\n",
    "    Args:\n",
    "        state (OverallState): The current state containing contents\n",
    "                              and token_max.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of `Send` objects with the name of a node in the graph\n",
    "              and the state to send to that node.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Send(\n",
    "            \"generate_summary\",\n",
    "            {\n",
    "                \"content\": content,\n",
    "                \"token_max\": state[\"token_max\"]\n",
    "            }\n",
    "        ) for content in state[\"contents\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "# Construct the graph\n",
    "# Nodes:\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_summary\", generate_summary)  # same as before\n",
    "graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "\n",
    "# Edges:\n",
    "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "graph.add_edge(\"generate_final_summary\", END)\n",
    "\n",
    "workflow = graph.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View & Validate Orchestration Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(workflow.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE_SUMMARY\n",
      "Used: 566 Expected: 2000\n",
      "Used: 892 Expected: 2000\n",
      "Used: 1173 Expected: 2000\n",
      "COLLECT_SUMMARIES\n",
      "Used: 566 Expected: 2000\n",
      "Used: 892 Expected: 2000\n",
      "Used: 1173 Expected: 2000\n",
      "COLLAPSE_SUMMARIES\n",
      "Used: 566 Expected: 2000\n",
      "GENERATE_FINAL_SUMMARY\n",
      "Used: 398 Expected: 2000\n",
      "['contents', 'token_max', 'summaries', 'collapsed_summaries', 'final_summary']\n"
     ]
    }
   ],
   "source": [
    "req = {\"contents\": [doc.page_content for doc in [RAW_DOC]],\n",
    "       \"token_max\": 2000}\n",
    "\n",
    "ouput = await workflow.ainvoke(\n",
    "    input=req,\n",
    "    config={\"recursion_limit\": 10},\n",
    ")\n",
    "print(list(ouput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Nikola Tesla, a Serbian-American inventor and engineer, revolutionized the world with his groundbreaking contributions to the development of the alternating current (AC) electrical system. Born in 1856, Tesla's passion for science and his prodigious intellect led him to the United States in 1884, where he worked with Thomas Edison before establishing his own laboratories and companies. His work on the AC induction motor and related patents, licensed by Westinghouse Electric, earned him significant wealth and became the cornerstone of the polyphase system.\n",
       "\n",
       "Driven by a relentless pursuit of innovation, Tesla conducted numerous experiments with mechanical oscillators, electric discharge tubes, and radiography. He also built a remote-controlled boat, showcasing his inventive spirit. His fame grew, and he became known for his public speaking abilities, captivating audiences with his visionary ideas.\n",
       "\n",
       "Tesla's vision extended beyond the realm of AC power. He pursued wireless lighting and global wireless power distribution through his high-voltage, high-frequency experiments. He envisioned a world powered by wireless energy, a concept that was ahead of its time. His unfinished Wardenclyffe Tower project, an intercontinental wireless transmitter, aimed to realize this vision but was hampered by financial constraints.\n",
       "\n",
       "Despite financial struggles and the failure of some ambitious projects, Tesla continued to experiment with various inventions throughout his life. He died in 1943, leaving behind a legacy of innovation and eccentricity. His work fell into relative obscurity until 1960, when the SI unit of magnetic flux density was named the tesla in his honor.\n",
       "\n",
       "Tesla's life was a testament to his relentless pursuit of innovation and his visionary ideas that often outpaced the technological capabilities of his time. His contributions to the AC system revolutionized the way we generate and distribute electricity, while his experiments with wireless power transmission and communication laid the groundwork for future technologies. His legacy continues to inspire and fascinate, reminding us of the power of imagination and the enduring impact of a brilliant and eccentric inventor. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(\n",
    "ouput[\"final_summary\"]\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
