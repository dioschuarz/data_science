{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUeTuzjfzRUlAhr90d7Y5K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dioschuarz/data_science/blob/main/llm/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libs"
      ],
      "metadata": {
        "id": "UDm7FQX1DpK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we install all libs we will need to do this model"
      ],
      "metadata": {
        "id": "EuOsJe0NfVZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n_SyrGAP6UT"
      },
      "outputs": [],
      "source": [
        "%pip install \\\n",
        "    datasets==2.11.0  --quiet \\\n",
        "    PyMuPDF==1.22.5 --quiet \\\n",
        "    langchain --quiet \\\n",
        "    chromadb --quiet \\\n",
        "    sentence_transformers --quiet \\\n",
        "    pypdf --quiet \\\n",
        "    faiss-gpu --quiet \\\n",
        "    git+https://www.github.com/huggingface/transformers --quiet \\\n",
        "    git+https://github.com/huggingface/accelerate --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libs"
      ],
      "metadata": {
        "id": "gRt5Br6MDs69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just import libs for modeling"
      ],
      "metadata": {
        "id": "_TzPmZAxfyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain.chains import RetrievalQA, question_answering, ConversationalRetrievalChain\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.schema import retriever\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import HuggingFaceHub\n",
        "import os"
      ],
      "metadata": {
        "id": "v_n3XazuQLdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('token') as f:\n",
        "  os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = f.read()"
      ],
      "metadata": {
        "id": "J5FWoEs2EOh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "7YPr4wIUDwCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your Google Drive folder"
      ],
      "metadata": {
        "id": "8DqzvA30geZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "X06F5wyiQMka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without prompt"
      ],
      "metadata": {
        "id": "u36i9hxC41Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's read the PDF with PyMuPDF and create an object with all this text in this PDF."
      ],
      "metadata": {
        "id": "ndg9YFC3gv6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from the PDF bytes\n",
        "\n",
        "pdf_text = \"\"\n",
        "pdf_document = fitz.open(f'/content/drive/MyDrive/Colab Notebooks/LLM/data/Prospecto_Definitivo.pdf', filetype=\"pdf\")\n",
        "for page_num in range(pdf_document.page_count):\n",
        "    page = pdf_document.load_page(page_num)\n",
        "    #pdf_text.append(page.get_text(\"text\"))\n",
        "    pdf_text += page.get_text(\"text\")\n",
        "\n",
        "pdf_document.close()\n",
        "\n",
        "# Now 'pdf_text' contains the extracted text from the PDF\n",
        "#print(pdf_text)"
      ],
      "metadata": {
        "id": "jAk_8npr5VDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's split this string object in smaller objects to make it easier to be read by the model"
      ],
      "metadata": {
        "id": "xoqVNMo1g8kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "                                      chunk_size=500,\n",
        "                                      chunk_overlap=25)\n",
        "\n",
        "chunks = text_splitter.split_text(pdf_text)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "vectorStore = FAISS.from_texts(chunks, embeddings)"
      ],
      "metadata": {
        "id": "fWEOltAH5TdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the file preprocessed, now let's load the model from HuggingFace"
      ],
      "metadata": {
        "id": "bUNW0BTnhMBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b\",\n",
        "                     model_kwargs={\"temperature\":0.1,\n",
        "                                   \"top_k\":10,\n",
        "                                   \"max_length\":512,\n",
        "                                   \"num_return_sequences\":1})\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                    chain_type=\"stuff\",\n",
        "                                    retriever=vectorStore.as_retriever())"
      ],
      "metadata": {
        "id": "65TjwRXx47X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know the answer from the model will generate a complete Q&A answer, that coul include more tokens, with more questions than what we are as asking to this model, so let's treat this answer!"
      ],
      "metadata": {
        "id": "i6vxLrHdhU-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(qachain, query):\n",
        "\n",
        "  answer = qachain({\"query\": query})\n",
        "\n",
        "  return answer['result'].strip().split('Question:')[0]"
      ],
      "metadata": {
        "id": "JnskKrMjMxvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now just try your questions to this model!"
      ],
      "metadata": {
        "id": "HGhMYEeeh65O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Qual o código do ativo na B3?\"\n",
        "\n",
        "answer = get_answer(chain, question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "s4x8W5Ah5EU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Qual o valor total da oferta?\"\n",
        "\n",
        "answer = get_answer(chain, question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "dn2xxLae5J2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Qual o maior risco da oferta?\"\n",
        "\n",
        "answer = get_answer(chain, question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyHz5lhx5Kps",
        "outputId": "a51959e3-1753-4b2b-950f-6912557928e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A maior parte do risco está relacionada com a liquidação da oferta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Com Prompt"
      ],
      "metadata": {
        "id": "q4Bqbs9J47oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we do again the same preprocessing, but the change will happen in the model, we will pass a prompt."
      ],
      "metadata": {
        "id": "h_318zwfiLtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from the PDF bytes\n",
        "\n",
        "pdf_text = \"\"\n",
        "pdf_document = fitz.open(f'/content/drive/MyDrive/Colab Notebooks/LLM/data/Prospecto_Definitivo.pdf', filetype=\"pdf\")\n",
        "for page_num in range(pdf_document.page_count):\n",
        "    page = pdf_document.load_page(page_num)\n",
        "    pdf_text += page.get_text(\"text\")\n",
        "\n",
        "pdf_document.close()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "                                      chunk_size=500,\n",
        "                                      chunk_overlap=25)\n",
        "\n",
        "chunks = text_splitter.split_text(pdf_text)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "docsearch = FAISS.from_texts(chunks, embeddings)\n",
        "\n",
        "# Prepare embedding model\n",
        "retriever = Chroma(persist_directory=\"./data\",\n",
        "                   embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "2z0YQgFJfbak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we do before, we load the model from HuggingFace, here we just added some arguments to improve the answer with our Prompt."
      ],
      "metadata": {
        "id": "gCU7Z0vXiW6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Falcon Huggingface API\n",
        "llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b\",\n",
        "            model_kwargs = {\n",
        "                \"max_length\":512,\n",
        "                \"max_new_tokens\":300,\n",
        "                \"min_new_tokens\":5,\n",
        "                \"temperature\":0.1,\n",
        "                \"repetition_penalty\": 1.5,\n",
        "                \"top_k\":1\n",
        "            }\n",
        "      )"
      ],
      "metadata": {
        "id": "BWz31M7uewWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create an Prompt!"
      ],
      "metadata": {
        "id": "ohTf1Pvpilxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare stuff prompt template\n",
        "prompt_template = \"\"\"\n",
        "You are a talkative AI assistant. Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to\n",
        "make up an answer.\n",
        "\n",
        "Answer all user questions using at maximum 500 characters.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template\n",
        ")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\" : prompt}\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=docsearch.as_retriever(),\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs=chain_type_kwargs\n",
        "    )"
      ],
      "metadata": {
        "id": "mA_F_loVeZOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, as this model we know wil get a complete answer using all the tokens we setted, we will treat this answer to give only our answer."
      ],
      "metadata": {
        "id": "QQhmSljLiqIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(qachain, query):\n",
        "\n",
        "  answer = qachain({\"query\": query})\n",
        "\n",
        "  return answer['result'].strip().split('Question:')[0]"
      ],
      "metadata": {
        "id": "OA_XPg6BGHxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now just try your questions to the model!"
      ],
      "metadata": {
        "id": "UH9r_W9zi6L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question =  \"Qual o código do ativo na B3?\"\n",
        "\n",
        "answer = get_answer(chain, question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "bZyohfIpdo5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question =  \"Qual o valor da oferta em reais?\"\n",
        "\n",
        "answer = get_answer(chain, question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "446fxdYjgjDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Explique qual o maior risco da oferta\"\n",
        "\n",
        "answer = get_answer(chain, question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "Qi9axs2ukYEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Qual o custo da comissão de estruturação total?\"\n",
        "\n",
        "answer = get_answer(chain, question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "SJuEOQuCmFJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Qual a política de investimentos?\"\n",
        "\n",
        "answer = get_answer(chain, question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "7oJ25GnumeiQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}