{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dioschuarz/data_science/blob/main/llm/openai_wikipedia_semantic_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661cd7c3",
      "metadata": {
        "id": "661cd7c3",
        "outputId": "67266b54-33cf-4d49-eeaf-d24b1b8ce84e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/73.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iS-BFPg7nDrj"
      },
      "id": "iS-BFPg7nDrj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "18334cc9",
      "metadata": {
        "id": "18334cc9"
      },
      "source": [
        "## First let's talk directly to ChatGPT and try and get back a response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b4c268",
      "metadata": {
        "id": "94b4c268"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# models\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "GPT_MODEL = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf463bfd",
      "metadata": {
        "id": "bf463bfd"
      },
      "outputs": [],
      "source": [
        "openai.api_key = '<your_api_key>'\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=GPT_MODEL,\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the gold medal for curling in Olymics 2022?\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec9b3016",
      "metadata": {
        "id": "ec9b3016"
      },
      "source": [
        "# Get the data about Winter Olympics and provide the information to ChatGPT as context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d66c821",
      "metadata": {
        "id": "1d66c821"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db8c9ea",
      "metadata": {
        "id": "6db8c9ea"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib --quiet\n",
        "!pip install plotly.express --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "!pip install tabulate --quiet\n",
        "!pip install tiktoken --quiet\n",
        "!pip install wget --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2ff949",
      "metadata": {
        "id": "2e2ff949"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import wget\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee04060",
      "metadata": {
        "id": "fee04060"
      },
      "source": [
        "## Step 1 - Grab the data from CSV and prepare it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db5b6960",
      "metadata": {
        "id": "db5b6960"
      },
      "outputs": [],
      "source": [
        "# download pre-chunked text and pre-computed embeddings\n",
        "# this file is ~200 MB, so may take a minute depending on your connection speed\n",
        "embeddings_path = \"https://cdn.openai.com/API/examples/data/winter_olympics_2022.csv\"\n",
        "file_path = \"winter_olympics_2022.csv\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    wget.download(embeddings_path, file_path)\n",
        "    print(\"File downloaded successfully.\")\n",
        "else:\n",
        "    print(\"File already exists in the local file system.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "537c70e0",
      "metadata": {
        "id": "537c70e0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\n",
        "    \"winter_olympics_2022.csv\"\n",
        ")\n",
        "\n",
        "# convert embeddings from CSV str type back to list type\n",
        "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6532e3f",
      "metadata": {
        "id": "d6532e3f"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "862b6ec1",
      "metadata": {
        "id": "862b6ec1"
      },
      "outputs": [],
      "source": [
        "df.info(show_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52d9f394",
      "metadata": {
        "id": "52d9f394"
      },
      "source": [
        "## Step 2 - Set up SingleStore DB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "%reload_ext sql\n",
        "%sql sqlite://new_db.db"
      ],
      "metadata": {
        "id": "j1xoMm1WK-9L",
        "outputId": "b2562821-ddaa-4f88-8d65-a9a0b0f83f2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "j1xoMm1WK-9L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection info needed in SQLAlchemy format, example:\n",
            "               postgresql://username:password@hostname/dbname\n",
            "               or an existing connection: dict_keys([])\n",
            "Invalid SQLite URL: sqlite://new_db.db\n",
            "Valid SQLite URL forms are:\n",
            " sqlite:///:memory: (or, sqlite://)\n",
            " sqlite:///relative/path/to/file.db\n",
            " sqlite:////absolute/path/to/file.db\n",
            "Connection info needed in SQLAlchemy format, example:\n",
            "               postgresql://username:password@hostname/dbname\n",
            "               or an existing connection: dict_keys([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20e92c6",
      "metadata": {
        "id": "c20e92c6",
        "outputId": "c31eecc2-800b-4c56-dcfe-80dd63d92329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variable $DATABASE_URL not set, and no connect string given.\n",
            "Connection info needed in SQLAlchemy format, example:\n",
            "               postgresql://username:password@hostname/dbname\n",
            "               or an existing connection: dict_keys([])\n"
          ]
        }
      ],
      "source": [
        "%%sql\n",
        "-- Create the database\n",
        "DROP DATABASE IF EXISTS winter_wikipedia;\n",
        "CREATE DATABASE IF NOT EXISTS winter_wikipedia;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac0bbdb",
      "metadata": {
        "id": "1ac0bbdb",
        "outputId": "6b24ba5c-873e-453c-ce76-6b7da8ddd9ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variable $DATABASE_URL not set, and no connect string given.\n",
            "Connection info needed in SQLAlchemy format, example:\n",
            "               postgresql://username:password@hostname/dbname\n",
            "               or an existing connection: dict_keys([])\n"
          ]
        }
      ],
      "source": [
        "%%sql\n",
        "USE winter_wikipedia;\n",
        "CREATE TABLE IF NOT EXISTS winter_olympics_2022 (\n",
        "    id INT PRIMARY KEY,\n",
        "    text TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "    embedding BLOB\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7772495d",
      "metadata": {
        "id": "7772495d"
      },
      "source": [
        "## Step 3 - Populate the Table with our dataframe df and use JSON_ARRAY_PACK to compact it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7578637c",
      "metadata": {
        "id": "7578637c"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import *\n",
        "\n",
        "db_connection = create_engine(connection_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15397c5",
      "metadata": {
        "id": "a15397c5"
      },
      "outputs": [],
      "source": [
        "def insert_records(\n",
        "    df: pd.DataFrame,\n",
        "    table_name: str,\n",
        "    batch_size: int = 1000\n",
        "):\n",
        "\n",
        "    stmt = f\"\"\"\n",
        "        INSERT INTO {table_name} (\n",
        "            id,\n",
        "            text,\n",
        "            embedding\n",
        "        )\n",
        "        VALUES (\n",
        "            %s,\n",
        "            %s,\n",
        "            JSON_ARRAY_PACK_F64(%s)\n",
        "        )\n",
        "    \"\"\".format(table_name=table_name)\n",
        "\n",
        "    record_arr = df.to_records(index=True)\n",
        "\n",
        "    for i in range(0, len(record_arr), batch_size):\n",
        "        batch = record_arr[i:i+batch_size]\n",
        "        values = [(row[0], row[1], str(row[2])) for row in batch]\n",
        "        db_connection.execute(stmt, values)\n",
        "    return\n",
        "\n",
        "insert_records(df, \"winter_olympics_2022\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381f68a0",
      "metadata": {
        "id": "381f68a0"
      },
      "source": [
        "## Step 4 - Do a semantic search with the same question from above and use the response to send to OpenAI again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7abbf1c",
      "metadata": {
        "id": "a7abbf1c"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding\n",
        "\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    table_name: str,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 100\n",
        ") -> tuple:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "\n",
        "    # Get the embedding of the query.\n",
        "    query_embedding_response = get_embedding(query, EMBEDDING_MODEL)\n",
        "\n",
        "    # Create the SQL statement.\n",
        "    stmt = f\"\"\"\n",
        "        SELECT\n",
        "            text,\n",
        "            DOT_PRODUCT_F64(JSON_ARRAY_PACK_F64(%s), embedding) AS score\n",
        "        FROM {table_name}\n",
        "        ORDER BY score DESC\n",
        "        LIMIT %s\n",
        "    \"\"\".format(table_name=table_name)\n",
        "\n",
        "    # Execute the SQL statement.\n",
        "    results = db_connection.execute(stmt, [str(query_embedding_response), top_n])\n",
        "\n",
        "    strings = []\n",
        "    relatednesses = []\n",
        "\n",
        "    for row in results:\n",
        "        strings.append(row[0])\n",
        "        relatednesses.append(row[1])\n",
        "\n",
        "    # Return the results.\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca909193",
      "metadata": {
        "id": "ca909193"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "strings, relatednesses = strings_ranked_by_relatedness(\n",
        "    \"curling gold medal\",\n",
        "    df,\n",
        "    \"winter_olympics_2022\",\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "for string, relatedness in zip(strings, relatednesses):\n",
        "    print(f\"{relatedness=:.3f}\")\n",
        "    print(tabulate([[string]], headers=['Result'], tablefmt='fancy_grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bdfb3d6",
      "metadata": {
        "id": "6bdfb3d6"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str,\n",
        "    token_budget: int\n",
        ") -> str:\n",
        "    \"\"\"Return a message for GPT, with relevant source texts pulled from SingleStoreDB.\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df, \"winter_olympics_2022\")\n",
        "    introduction = 'Use the below articles on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "    message = introduction\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (\n",
        "            num_tokens(message + next_article + question, model=model)\n",
        "            > token_budget\n",
        "        ):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "\n",
        "def ask(\n",
        "    query: str,\n",
        "    df: pd.DataFrame = df,\n",
        "    model: str = GPT_MODEL,\n",
        "    token_budget: int = 4096 - 500,\n",
        "    print_message: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Answers a query using GPT and a table of relevant texts and embeddings in SingleStoreDB.\"\"\"\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    if print_message:\n",
        "        print(message)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You answer questions about the 2022 Winter Olympics.\"},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return response_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75242051",
      "metadata": {
        "id": "75242051"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "answer = ask('Who won the gold medal for curling in Olymics 2022?')\n",
        "\n",
        "pprint(answer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
